#!/usr/bin/env python
"""
Generate plots from Train-Synthetic-Test-Real (TSTR) evaluation results.

This script reads previously generated evaluation results from CSV files
and creates visualizations without needing to rerun the evaluation process.
"""

import os
import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Default figure format (will be overridden by CLI argument)
FIG_FORMAT = "png"


def setup_argparse():
    """Set up command line argument parser"""
    parser = argparse.ArgumentParser(
        description="Generate plots from TSTR evaluation results"
    )
    
    # Required arguments
    parser.add_argument(
        "--results_dir",
        type=str,
        required=True,
        help="Directory containing evaluation results CSV files"
    )
    
    # Optional arguments
    parser.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="Directory to save generated plots (defaults to results_dir/plots)"
    )
    parser.add_argument(
        "--targets",
        type=str,
        nargs="+",
        default=None,
        help="Target variables to plot (defaults to all available in the data)"
    )
    parser.add_argument(
        "--prediction_modes",
        type=str,
        nargs="+",
        default=None,
        help="Prediction modes to plot (defaults to all available in the data)"
    )
    parser.add_argument(
        "--plot_types",
        type=str,
        nargs="+",
        default=["performance", "utility", "feature_importance", "feature_alignment"],
        choices=["performance", "utility", "feature_importance", "feature_alignment"],
        help="Types of plots to generate"
    )
    parser.add_argument(
        "--top_n_features",
        type=int,
        default=15,
        help="Number of top features to show in feature importance plots"
    )
    parser.add_argument(
        "--format",
        dest="fig_format",
        type=str,
        choices=["png", "pdf", "jpg"],
        default="pdf",
        help="Format of the saved figures"
    )
    
    return parser

def get_custom_color_palette(num_colors, exclude_real=False):
    """
    Get a custom color palette with visually appealing colors
    
    Args:
        num_colors (int): Number of colors needed
        exclude_real (bool): Whether to reserve a special color for 'Real' dataset
        
    Returns:
        list: List of colors in RGB tuples
    """
    # Define a visually appealing color palette
    custom_colors = [
        (0.00, 0.63, 0.64),  # Teal
        (0.75, 0.12, 0.24),  # Crimson
        (0.85, 0.37, 0.01),  # Dark orange
        (0.13, 0.55, 0.13),  # Forest green
        (0.80, 0.52, 0.25),  # Ochre
        (0.55, 0.00, 0.26),  # Burgundy
        (0.00, 0.21, 0.41),  # Navy
        (0.64, 0.08, 0.18),  # Dark red
        (0.00, 0.39, 0.25),  # Dark green
        (0.55, 0.27, 0.07),  # Brown
        (0.28, 0.24, 0.55)   # Dark slate
    ]
    
    if exclude_real:
        real_color = (0.00, 0.27, 0.55)  # Deep blue
        required_colors = num_colors - 1
        if required_colors > len(custom_colors):
            additional_colors = sns.color_palette("tab20", required_colors)
            result_colors = custom_colors + additional_colors
            result_colors = result_colors[:required_colors]
        else:
            result_colors = custom_colors[:required_colors]
        return [real_color] + result_colors
    else:
        if num_colors > len(custom_colors):
            additional_colors = sns.color_palette("tab20", num_colors)
            result_colors = custom_colors + additional_colors
            return result_colors[:num_colors]
        else:
            return custom_colors[:num_colors]


def create_feature_label_mapping():
    """Mapping between technical column names and human-readable labels"""
    return {
        'IATA_CARRIER_CODE': 'Carrier',
        'DEPARTURE_IATA_AIRPORT_CODE': 'Departure Airport',
        'ARRIVAL_IATA_AIRPORT_CODE': 'Arrival Airport',
        'AIRCRAFT_TYPE_IATA': 'Aircraft Type',
        'AIRPORT_PAIR': 'Airport Pair',
        'SCHEDULED_MONTH': 'Scheduled Month',
        'SCHEDULED_DAY': 'Scheduled Day',
        'SCHEDULED_HOUR': 'Scheduled Hour',
        'SCHEDULED_MINUTE': 'Scheduled Minute',
        'SCHEDULED_DURATION_MIN': 'Scheduled Duration',
        'ACTUAL_DURATION_MIN': 'Actual Duration',
        'DURATION_DIFF_MIN': 'Duration Difference',
        'DAY_OF_WEEK': 'Day of Week',
        'ARRIVAL_HOUR': 'Arrival Hour',
        'ARRIVAL_DAY': 'Arrival Day',
        'DEPARTURE_DELAY_MIN': 'Departure Delay',
        'ARRIVAL_DELAY_MIN': 'Arrival Delay',
        'TURNAROUND_MIN': 'Turnaround Time'
    }


def get_readable_title(target, mode):
    """Get human-readable title for plots"""
    label_mapping = create_feature_label_mapping()
    readable_target = label_mapping.get(target, target.replace('_', ' ').title())
    mode_str = mode.title()
    return f"{readable_target} Prediction ({mode_str})"


def plot_performance_metrics(performance_df, target, mode, output_dir):
    """Plot performance metrics for the given target and prediction mode"""
    filtered_df = performance_df[
        (performance_df['Target'] == target) & 
        (performance_df['Mode'] == mode)
    ]
    if filtered_df.empty:
        print(f"No performance data available for {target} with {mode} mode")
        return
    
    title_base = get_readable_title(target, mode)
    target_mode_dir = os.path.join(output_dir, f"{target.lower()}_{mode}")
    os.makedirs(target_mode_dir, exist_ok=True)
    
    metrics = [
        ('RMSE', 'Root Mean Squared Error (lower is better)'),
        ('MAE', 'Mean Absolute Error (lower is better)'),
        ('R2', 'RÂ² Score (higher is better)')
    ]
    
    for metric, metric_title in metrics:
        plt.figure(figsize=(14, 8))
        models = filtered_df['Model'].unique()
        datasets = filtered_df['Dataset'].unique()
        x = np.arange(len(models))
        width = 0.8 / len(datasets)
        has_real = 'Real' in datasets
        colors = get_custom_color_palette(len(datasets), exclude_real=has_real)
        
        for i, dataset in enumerate(datasets):
            dataset_data = filtered_df[filtered_df['Dataset'] == dataset]
            values = []
            for model in models:
                model_data = dataset_data[dataset_data['Model'] == model]
                values.append(model_data[metric].values[0] if not model_data.empty else 0)
            color_index = 0 if (dataset == 'Real' and has_real) else i
            plt.bar(x + i*width, values, width, label=dataset, color=colors[color_index])
        
        plt.xlabel('Model', fontsize=12)
        plt.ylabel(metric_title, fontsize=12)
        plt.title(f'{title_base} - {metric_title}', fontsize=14)
        plt.xticks(x + width*(len(datasets)-1)/2, models)
        plt.grid(axis='y', alpha=0.3)
        plt.legend(title='Dataset', title_fontsize=12, fontsize=10, loc='best')
        plt.tight_layout()
        
        for i, dataset in enumerate(datasets):
            dataset_data = filtered_df[filtered_df['Dataset'] == dataset]
            for j, model in enumerate(models):
                model_data = dataset_data[dataset_data['Model'] == model]
                if not model_data.empty:
                    value = model_data[metric].values[0]
                    fmt = f'{value:.2f}' if metric == 'R2' else f'{value:.1f}'
                    plt.text(
                        x[j] + i*width,
                        value + 0.01 * max(values),
                        fmt,
                        ha='center',
                        va='bottom',
                        fontsize=9
                    )
        
        # Save the plot in the requested format
        plt.savefig(
            os.path.join(
                target_mode_dir,
                f"{target.lower()}_{mode}_{metric.lower()}.{FIG_FORMAT}"
            ),
            format=FIG_FORMAT,
            dpi=300,
            bbox_inches='tight'
        )
        plt.close()


def plot_utility_scores(utility_df, target, mode, output_dir):
    """Plot utility scores for the given target and prediction mode"""
    filtered_df = utility_df[
        (utility_df['Target'] == target) & 
        (utility_df['Mode'] == mode)
    ]
    if filtered_df.empty:
        print(f"No utility data available for {target} with {mode} mode")
        return
    
    title_base = get_readable_title(target, mode)
    target_mode_dir = os.path.join(output_dir, f"{target.lower()}_{mode}")
    os.makedirs(target_mode_dir, exist_ok=True)
    
    datasets = filtered_df['Dataset'].unique()
    models = filtered_df['Model'].unique()
    colors = get_custom_color_palette(len(datasets))
    
    # Heatmap of overall utility scores
    plt.figure(figsize=(12, 9))
    heatmap_data = filtered_df.pivot(index='Model', columns='Dataset', values='Overall Utility')
    heatmap_data = heatmap_data[datasets]
    heatmap_values = heatmap_data.values
    ordered_heatmap = pd.DataFrame(heatmap_values, index=models, columns=datasets)
    cmap = plt.cm.YlGnBu
    ax = sns.heatmap(
        ordered_heatmap,
        annot=True,
        cmap=cmap,
        vmin=0,
        vmax=1,
        fmt='.2f',
        cbar_kws={'label': 'Utility Score (higher is better)'},
        linewidths=0.5,
        linecolor='white',
        annot_kws={"size": 12}
    )
    plt.title(f'{title_base} - Synthetic Data Utility Scores', fontsize=14, pad=20)
    ax.set_xlabel('Synthetic Dataset', fontsize=14, labelpad=10)
    ax.set_ylabel('Model', fontsize=14, labelpad=10)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    for _, spine in ax.spines.items():
        spine.set_visible(True)
        spine.set_linewidth(1.0)
    plt.tight_layout()
    plt.savefig(
        os.path.join(
            target_mode_dir,
            f"{target.lower()}_{mode}_utility_heatmap.{FIG_FORMAT}"
        ),
        format=FIG_FORMAT,
        dpi=300,
        bbox_inches='tight'
    )
    plt.close()
    
    # Bar chart of average utility
    plt.figure(figsize=(10, 7))
    avg_utility = [
        {'Dataset': ds, 'Overall Utility': filtered_df[filtered_df['Dataset'] == ds]['Overall Utility'].mean()}
        for ds in datasets
    ]
    avg_df = pd.DataFrame(avg_utility)
    bars = plt.bar(
        avg_df['Dataset'],
        avg_df['Overall Utility'],
        color=[colors[i] for i in range(len(datasets))],
        width=0.4
    )
    plt.title(f'{title_base} - Average Synthetic Data Utility', fontsize=16)
    plt.xlabel('Synthetic Dataset', fontsize=14)
    plt.ylabel('Average Utility Score', fontsize=14)
    plt.ylim(0, 1)
    plt.grid(axis='y', alpha=0.3)
    plt.xticks(fontsize=12)
    plt.yticks(fontsize=12)
    for bar in bars:
        height = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            height + 0.02,
            f'{height:.2f}',
            ha='center',
            fontsize=12
        )
    plt.tight_layout()
    plt.savefig(
        os.path.join(
            target_mode_dir,
            f"{target.lower()}_{mode}_avg_utility.{FIG_FORMAT}"
        ),
        format=FIG_FORMAT,
        dpi=300,
        bbox_inches='tight'
    )
    plt.close()


def plot_feature_importances(importance_df, target, mode, output_dir, top_n=15):
    """Plot feature importances for the given target and prediction mode"""
    filtered_df = importance_df[
        (importance_df['Target'] == target) & 
        (importance_df['Mode'] == mode)
    ]
    if filtered_df.empty:
        print(f"No feature importance data available for {target} with {mode} mode")
        return
    
    title_base = get_readable_title(target, mode)
    importance_dir = os.path.join(output_dir, f"{target.lower()}_{mode}", "feature_importances")
    os.makedirs(importance_dir, exist_ok=True)
    
    label_mapping = create_feature_label_mapping()
    models = filtered_df['Model'].unique()
    datasets = filtered_df['Dataset'].unique()
    has_real = 'Real' in datasets
    colors = get_custom_color_palette(len(datasets), exclude_real=has_real)
    
    # Per-model feature comparisons
    for model in models:
        model_df = filtered_df[filtered_df['Model'] == model]
        if model_df.empty:
            continue
        
        feature_avg = model_df.groupby('Feature')['Importance'].mean().reset_index()
        top_features = feature_avg.nlargest(top_n, 'Importance')['Feature'].tolist()
        comp_df = pd.DataFrame(index=top_features)
        for ds in datasets:
            ds_df = model_df[model_df['Dataset'] == ds]
            comp_df[ds] = ds_df.set_index('Feature')['Importance'].reindex(top_features, fill_value=0)
        
        comp_df['avg'] = comp_df.mean(axis=1)
        comp_df = comp_df.sort_values('avg', ascending=True).drop('avg', axis=1)
        
        fig, ax = plt.subplots(figsize=(12, 10))
        reversed_colors = colors[::-1]
        comp_df[comp_df.columns[::-1]].plot(
            kind='barh',
            ax=ax,
            color=[reversed_colors[i % len(reversed_colors)] for i in range(len(comp_df.columns))],
            width=0.8
        )
        
        readable_labels = []
        for feat in comp_df.index:
            label = feat
            if feat in label_mapping:
                label = label_mapping[feat]
            elif any(feat.startswith(k + '_') for k in label_mapping):
                for k in label_mapping:
                    if feat.startswith(k + '_'):
                        cat = feat[len(k) + 1:]
                        label = f"{label_mapping[k]}: {cat}"
                        break
            readable_labels.append(label)
        ax.set_yticklabels(readable_labels, fontsize=10)
        ax.set_xlim(0, comp_df.values.max() * 1.1)
        plt.title(f'Feature Importance - {model}\n{title_base}', fontsize=14)
        plt.xlabel('Normalized Importance', fontsize=12)
        handles, labels = ax.get_legend_handles_labels()
        ax.legend(handles[::-1], labels[::-1], title='Dataset', loc='lower right', fontsize=10)
        plt.grid(axis='x', alpha=0.3)
        plt.tight_layout()
        plt.savefig(
            os.path.join(
                importance_dir,
                f"{target.lower()}_{mode}_{model.replace(' ', '_')}_feature_comparison.{FIG_FORMAT}"
            ),
            format=FIG_FORMAT,
            dpi=300,
            bbox_inches='tight'
        )
        plt.close()
    
    # Average across all models
    avg_imp = filtered_df.groupby('Feature')['Importance'].mean().reset_index()
    top_all = avg_imp.nlargest(top_n, 'Importance')['Feature']
    all_comp = pd.DataFrame(index=top_all)
    for ds in datasets:
        ds_avg = filtered_df[filtered_df['Dataset'] == ds].groupby('Feature')['Importance'].mean()
        all_comp[ds] = ds_avg.reindex(all_comp.index, fill_value=0)
    all_comp['avg'] = all_comp.mean(axis=1)
    all_comp = all_comp.sort_values('avg', ascending=True).drop('avg', axis=1)
    
    fig, ax = plt.subplots(figsize=(12, 10))
    all_comp[all_comp.columns[::-1]].plot(
        kind='barh',
        ax=ax,
        color=[reversed_colors[i % len(reversed_colors)] for i in range(len(all_comp.columns))],
        width=0.8
    )
    readable_labels = []
    for feat in all_comp.index:
        label = feat
        if feat in label_mapping:
            label = label_mapping[feat]
        elif any(feat.startswith(k + '_') for k in label_mapping):
            for k in label_mapping:
                if feat.startswith(k + '_'):
                    cat = feat[len(k) + 1:]
                    label = f"{label_mapping[k]}: {cat}"
                    break
        readable_labels.append(label)
    ax.set_yticklabels(readable_labels, fontsize=10)
    ax.set_xlim(0, all_comp.values.max() * 1.1)
    plt.title(f'Average Feature Importance Across All Models\n{title_base}', fontsize=14)
    plt.xlabel('Normalized Importance', fontsize=12)
    handles, labels = ax.get_legend_handles_labels()
    ax.legend(handles[::-1], labels[::-1], title='Dataset', loc='lower right', fontsize=10)
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig(
        os.path.join(
            importance_dir,
            f"{target.lower()}_{mode}_all_models_feature_comparison.{FIG_FORMAT}"
        ),
        format=FIG_FORMAT,
        dpi=300,
        bbox_inches='tight'
    )
    plt.close()


def calculate_feature_importance_alignment(importance_df, target, mode, output_dir):
    """Calculate and plot feature importance alignment scores between real and synthetic data."""
    filtered_df = importance_df[
        (importance_df['Target'] == target) & 
        (importance_df['Mode'] == mode)
    ]
    if filtered_df.empty:
        print(f"No feature importance data available for {target} with {mode} mode")
        return None
    
    title_base = get_readable_title(target, mode)
    alignment_dir = os.path.join(output_dir, f"{target.lower()}_{mode}")
    os.makedirs(alignment_dir, exist_ok=True)
    
    models = filtered_df['Model'].unique()
    datasets = filtered_df['Dataset'].unique()
    if 'Real' not in datasets:
        print(f"Real dataset not found for {target} with {mode}")
        return None
    synthetic_datasets = [d for d in datasets if d != 'Real']
    if not synthetic_datasets:
        print(f"No synthetic datasets for {target} with {mode}")
        return None
    
    alignment_scores = []
    for model in models:
        model_df = filtered_df[filtered_df['Model'] == model]
        if model_df.empty:
            continue
        real_vec = model_df[model_df['Dataset'] == 'Real'].set_index('Feature')['Importance']
        for ds in synthetic_datasets:
            synth_vec = model_df[model_df['Dataset'] == ds].set_index('Feature')['Importance']
            all_feats = list(set(real_vec.index) | set(synth_vec.index))
            rv = np.array([real_vec.get(f, 0) for f in all_feats])
            sv = np.array([synth_vec.get(f, 0) for f in all_feats])
            cosine_sim = (
                np.dot(rv, sv) / (np.linalg.norm(rv) * np.linalg.norm(sv))
                if np.linalg.norm(rv) * np.linalg.norm(sv) > 0 else 0
            )
            corr = (
                np.corrcoef(rv, sv)[0, 1]
                if len(all_feats) > 1 else 0
            )
            if np.isnan(corr):
                corr = 0
            alignment_scores.append({
                'Target': target,
                'Mode': mode,
                'Model': model,
                'Dataset': ds,
                'CosineSimScore': cosine_sim,
                'CorrelationScore': corr,
                'NumFeatures': len(all_feats)
            })
    
    alignment_df = pd.DataFrame(alignment_scores)
    if alignment_df.empty:
        print(f"Could not calculate alignment for {target} with {mode}")
        return None
    
    avg_scores = alignment_df.groupby('Dataset')[['CosineSimScore', 'CorrelationScore']].mean().reset_index()
    
    # Heatmap of cosine similarity
    plt.figure(figsize=(12, 8))
    cos_heat = alignment_df.pivot(index='Model', columns='Dataset', values='CosineSimScore')
    cos_heat = cos_heat[synthetic_datasets]
    ax = sns.heatmap(
        cos_heat,
        annot=True,
        cmap=plt.cm.viridis,
        vmin=0,
        vmax=1,
        fmt='.2f',
        cbar_kws={'label': 'Cosine Similarity Score (higher is better)'},
        linewidths=0.5,
        linecolor='white',
        annot_kws={"size": 12}
    )
    plt.title(f'{title_base} - Feature Importance Alignment (Cosine Similarity)', fontsize=14)
    plt.xlabel('Synthetic Dataset', fontsize=12)
    plt.ylabel('Model', fontsize=12)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    for _, spine in ax.spines.items():
        spine.set_visible(True)
        spine.set_linewidth(1.0)
    plt.tight_layout()
    plt.savefig(
        os.path.join(
            alignment_dir,
            f"{target.lower()}_{mode}_cosine_alignment_heatmap.{FIG_FORMAT}"
        ),
        format=FIG_FORMAT,
        dpi=300,
        bbox_inches='tight'
    )
    plt.close()
    
    # Bar chart of average cosine similarity
    ordered_avg = pd.concat([
        avg_scores[avg_scores['Dataset'] == ds]
        for ds in synthetic_datasets
    ], ignore_index=True)
    plt.figure(figsize=(10, 6))
    bars = plt.bar(
        ordered_avg['Dataset'],
        ordered_avg['CosineSimScore'],
        color=get_custom_color_palette(len(synthetic_datasets)),
        width=0.4
    )
    for bar in bars:
        h = bar.get_height()
        plt.text(
            bar.get_x() + bar.get_width()/2.,
            h + 0.02,
            f'{h:.2f}',
            ha='center',
            fontsize=12
        )
    plt.title(f'{title_base} - Average Feature Importance Alignment', fontsize=14)
    plt.xlabel('Synthetic Dataset', fontsize=12)
    plt.ylabel('Cosine Similarity Score', fontsize=12)
    plt.ylim(0, 1.1)
    plt.grid(axis='y', alpha=0.3)
    plt.xticks(fontsize=11)
    plt.yticks(fontsize=11)
    plt.axhline(y=1.0, color='r', linestyle='--', alpha=0.5)
    plt.text(0.02, 1.02, 'Perfect Alignment', color='r', fontsize=10)
    plt.tight_layout()
    plt.savefig(
        os.path.join(
            alignment_dir,
            f"{target.lower()}_{mode}_avg_alignment_score.{FIG_FORMAT}"
        ),
        format=FIG_FORMAT,
        dpi=300,
        bbox_inches='tight'
    )
    plt.close()
    
    return alignment_df


def main():
    """Main function to generate plots from CSV results"""
    parser = setup_argparse()
    args = parser.parse_args()
    
    # Override global figure format
    global FIG_FORMAT
    FIG_FORMAT = args.fig_format
    
    if args.output_dir is None:
        args.output_dir = os.path.join(args.results_dir, "plots")
    os.makedirs(args.output_dir, exist_ok=True)
    
    print(f"Reading evaluation results from {args.results_dir}")
    plt.style.use('default')  # Reset to default style
    plt.rcParams.update({
        'font.family': 'sans-serif',
        'font.sans-serif': ['DejaVu Sans', 'Arial', 'Liberation Sans', 'Bitstream Vera Sans', 'sans-serif'],
        'font.weight': 'normal',
        'axes.facecolor': '#f8f8f8',
        'figure.facecolor': 'white',
        'figure.dpi': 100,
        'savefig.dpi': 300,
        'savefig.bbox': 'tight',
        'savefig.pad_inches': 0.1,
        'axes.labelweight': 'normal',
        'axes.titleweight': 'normal',
        'xtick.labelsize': 10,
        'ytick.labelsize': 10,
        'axes.labelsize': 12,
        'axes.titlesize': 14
    })
    
    performance_path = os.path.join(args.results_dir, 'all_performance_results.csv')
    utility_path = os.path.join(args.results_dir, 'all_utility_results.csv')
    if not os.path.exists(performance_path):
        raise FileNotFoundError(f"Performance results file not found at {performance_path}")
    if not os.path.exists(utility_path):
        raise FileNotFoundError(f"Utility results file not found at {utility_path}")
    
    performance_df = pd.read_csv(performance_path)
    utility_df = pd.read_csv(utility_path)
    
    importance_df = None
    if 'feature_importance' in args.plot_types:
        importance_path = os.path.join(args.results_dir, 'all_feature_importances.csv')
        if os.path.exists(importance_path):
            print(f"Loading feature importance data from {importance_path}")
            importance_df = pd.read_csv(importance_path)
        else:
            importance_dir = os.path.join(args.results_dir, 'feature_importances')
            if os.path.isdir(importance_dir):
                files = [f for f in os.listdir(importance_dir) if f.endswith('.csv')]
                if files:
                    dfs = [pd.read_csv(os.path.join(importance_dir, f)) for f in files]
                    importance_df = pd.concat(dfs, ignore_index=True)
                else:
                    print("No feature importance CSV files found. Skipping.")
            else:
                print("Feature importance data not found. Skipping.")
    
    targets = args.targets or performance_df['Target'].unique()
    modes = args.prediction_modes or performance_df['Mode'].unique()
    
    for target in targets:
        for mode in modes:
            if target == 'DEPARTURE_DELAY_MIN' and mode == 'tactical':
                print(f"Skipping {target} with {mode} mode (not applicable)")
                continue
            print(f"Generating plots for {target} ({mode} mode)")
            if 'performance' in args.plot_types:
                plot_performance_metrics(performance_df, target, mode, args.output_dir)
            if 'utility' in args.plot_types:
                plot_utility_scores(utility_df, target, mode, args.output_dir)
            if 'feature_importance' in args.plot_types and importance_df is not None:
                plot_feature_importances(importance_df, target, mode, args.output_dir, args.top_n_features)
            if 'feature_alignment' in args.plot_types and importance_df is not None:
                calculate_feature_importance_alignment(importance_df, target, mode, args.output_dir)
    
    print(f"All plots generated and saved to {args.output_dir}")


if __name__ == "__main__":
    main()


# Example usage:
# python scripts/generate_tstr_plots.py --results_dir tstr_results --output_dir tstr_plots