[How to evaluate the quality of the synthetic data â€“ measuring from the perspective of fidelity, utility, and privacy](https://aws.amazon.com/blogs/machine-learning/how-to-evaluate-the-quality-of-the-synthetic-data-measuring-from-the-perspective-of-fidelity-utility-and-privacy/)

The three dimensions of synthetic data quality evaluation: 

The synthetic data generated is measured against three key dimensions:

    Fidelity
    Utility
    Privacy

These are some of the questions about any generated synthetic data that should be answered by a synthetic data quality report:

    How similar is this synthetic data as compared to the original training set?
    How useful is this synthetic data for our downstream applications?
    Has any information been leaked from the original training data into the synthetic data?


## The different dimensions of synthetic data quality

### Fidelity
  The fidelity of synthetic data is determined by two factors: how closely the generated data matches the original data points and how much diversity is preserved in the newly created synthetic points.
Fidelity metrics assist us in determining how closely the generated data matches the original data, thus instilling confidence in its suitability for exploration and analysis.

### Utility
  The utility score measures how useful synthetic data can be in replacing real data for downstream applications, such as analytics or even machine learning models.
  
### Privacy
  Privacy indicates the level of confidentiality of synthetic data. When validating the privacy of generated synthetic data, it is important to classify the different bits of information according to the identification risk and value they hold for the downstream application. 

### Mapping of the SDMetrics â€œsingle-tableâ€ scores to the four high-level evaluation pillars

| Pillar | Metrics that belong here | Rationale |
|--------|-------------------------|-----------|
| **Fidelity**<br>*How closely does the synthetic table mimic the real tableâ€™s statistical structure?* | â€¢ **BNLikelihood**  <br>â€¢ **BNLogLikelihood**  <br>â€¢ **GMLogLikelihood**  <br>â€¢ **LogisticDetection**  <br>â€¢ **SVCDetection**  <br>â€¢ **CSTest**  <br>â€¢ **KSComplement**  <br>â€¢ **ContinuousKLDivergence**  <br>â€¢ **DiscreteKLDivergence** | Likelihood, KS, KL and chi-square tests measure overlap of the real and synthetic distributions (univariate or multivariate).<br>The two *Detection* metrics ask â€œcan a classifier tell them apart?â€, which is another fidelity lens. |
| **Utility**<br>*How well can models trained on synthetic data generalise to real-world data for the intended ML task?* | â€¢ **BinaryDecisionTreeClassifier**  <br>â€¢ **BinaryAdaBoostClassifier**  <br>â€¢ **BinaryLogisticRegression**  <br>â€¢ **BinaryMLPClassifier**  <br>â€¢ **MulticlassDecisionTreeClassifier**  <br>â€¢ **MulticlassMLPClassifier**  <br>â€¢ **LinearRegression**  <br>â€¢ **MLPRegressor**  <br>â€¢ **MLEfficacy** | These ML-Efficacy scores train a model **only on the synthetic rows** and report its performance on real rows, so they directly quantify downstream usefulness. |
| **Privacy**<br>*How hard is it for an attacker to infer secrets about the original data set?* | â€¢ **CategoricalCAP**  <br>â€¢ **CategoricalZeroCAP**  <br>â€¢ **CategoricalGeneralizedCAP**  <br>â€¢ **CategoricalKNN**  <br>â€¢ **CategoricalNB**  <br>â€¢ **CategoricalRF**  <br>â€¢ **CategoricalEnsemble**  <br>â€¢ **NumericalMLP**  <br>â€¢ **NumericalLR**  <br>â€¢ **NumericalSVR**  <br>â€¢ **NumericalRadiusNearestNeighbor** | All of these intentionally train an adversarial model on the synthetic data and report its success rate (or probability) when attacking real data. Lower success â‡’ stronger privacy. |




# ğŸ§© Mapping Metrics to Categories

## 1. **Fidelity (Statistical Similarity)**

Statistical Similarity (Data Fidelity)

This measures how closely the synthetic data mimics the real data's distributions, relationships, and statistical properties.

These metrics evaluate **how statistically similar** your synthetic data is to the real data.

| Metric | Description |
|:-------|:------------|
| **BNLikelihood** | How likely the synthetic data is under a Bayesian Network trained on real data. |
| **BNLogLikelihood** | Log-likelihood variant, same purpose as above. |
| **GMLogLikelihood** | Log-likelihood under a Gaussian Mixture model fitted to real data. |
| **CSTest** | Chi-Squared Test across categorical columns, compares distributions. |
| **KSComplement** | Kolmogorov-Smirnov test complement for numerical columns. |
| **ContinuousKLDivergence** | KL divergence between pairs of numerical columns. |
| **DiscreteKLDivergence** | KL divergence between pairs of discrete (categorical/boolean) columns. |


---

## 2. **Utility (Downstream ML Task Performance)**

Utility (Downstream Task Performance)

This tests if models trained on synthetic data are as useful as models trained on real data.

These metrics test **how useful** the synthetic data is for machine learning models.

| Metric | Description |
|:-------|:------------|
| **BinaryDecisionTreeClassifier** | Train decision tree on synthetic for binary classification. |
| **BinaryAdaBoostClassifier** | Train AdaBoost on synthetic for binary classification. |
| **BinaryLogisticRegression** | Train logistic regression on synthetic for binary classification. |
| **BinaryMLPClassifier** | Train MLP (neural network) on synthetic for binary classification. |
| **MulticlassDecisionTreeClassifier** | Train decision tree on synthetic for multiclass classification. |
| **MulticlassMLPClassifier** | Train MLP on synthetic for multiclass classification. |
| **LinearRegression** | Linear regression for regression tasks. |
| **MLPRegressor** | MLP for regression tasks. |
| **MLEfficacy** | A wrapper that detects task type automatically and applies appropriate metrics. |


---

## 3. **Privacy (Disclosure Risk)**
Privacy (Disclosure Risk)

Synthetic data should protect privacy â€” real individuals should not be easily re-identified.

These metrics assess **how vulnerable** the synthetic data is to privacy attacks.

| Metric | Description |
|:-------|:------------|
| **CategoricalCAP** | Correct Attribution Probability for categorical columns. |
| **CategoricalZeroCAP** | Variant of CAP for categorical columns. |
| **CategoricalGeneralizedCAP** | Generalized CAP metric. |
| **CategoricalKNN** | KNN model to guess real records. |
| **CategoricalNB** | Naive Bayes to guess real records. |
| **CategoricalRF** | Random Forest for attack simulation. |
| **CategoricalEnsemble** | Ensemble of models for stronger attacks. |
| **NumericalLR** | Linear Regression attack for numerical columns. |
| **NumericalMLP** | MLP attack for numerical columns. |
| **NumericalSVR** | Support Vector Regressor attack. |
| **NumericalRadiusNearestNeighbor** | Radius-based nearest neighbor attack on numerical data. |



---

## 4. **Diversity (Coverage of Distribution)**
Coverage and Diversity

This measures whether the synthetic data covers the full range of real-world scenarios, including rare cases.


However, **some Fidelity metrics can indirectly hint at diversity issues**, especially:
- **GMLogLikelihood**: If synthetic data collapses to a few modes, GMM log-likelihoods will be low.
- **ContinuousKLDivergence / DiscreteKLDivergence**: High KL divergence might signal missing modes.
- **CSTest** and **KSComplement**: If rare categories or tails of distributions are missing, these scores degrade.

---

## Detection

Tabular Detection describes a set of metrics that calculate how difficult it is to tell apart the real data from the synthetic data. 

This is done using a machine learning model. There are two different Detection metrics that use different ML algorithms: LogisticDetection and SVCDetection.



### How does it work?

- Create a single, augmented table that has all the rows of real data and all the rows of synthetic data. - Add an extra column to keep track of whether each original row is real or synthetic.
- Split the augmented data to create a training and validation sets.
- Choose a machine learning model based on the metric used (see below). Train the model on the training split. The model will predict whether each row is real or synthetic (ie predict the extra column we created in step #1)
- Validate the model on the validation set

### Score

The final score is based on the average ROC AUC score across all the cross validation splits.
$score=1âˆ’(max(ROC AUC,0.5)Ã—2âˆ’1)$


(highest) 1.0: The machine learning model cannot identify the synthetic data apart from the real data

(lowest) 0.0: The machine learning model can perfectly identify synthetic data apart from the real data

A score of 1 may indicate high quality but it could also be a clue that the synthetic data is leaking privacy (for example, if the synthetic data is copying the rows in the real data).



##

```
src/embeddings/evaluation/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ fidelity/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ distribution.py    # Distributional similarity metrics (KL, JS divergence)
â”‚   â”œâ”€â”€ detection.py       # Detection metrics (logistic, SVC)
â”‚   â”œâ”€â”€ likelihood.py      # Bayesian and GMM likelihood metrics
â”‚   â”œâ”€â”€ statistical.py     # Statistical tests (KS, CS tests)
â”‚   â””â”€â”€ correlation.py     # Correlation preservation metrics
â”œâ”€â”€ utility/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tstr.py            # Your existing TSTR implementation
â”‚   â”œâ”€â”€ ml_efficacy.py     # Machine Learning efficacy metrics
â”‚   â””â”€â”€ downstream_tasks.py # Performance on specific downstream tasks
â”œâ”€â”€ privacy/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cap.py             # Correct Attribution Probability methods
â”‚   â”œâ”€â”€ nn_attacks.py      # Nearest neighbor attacks
â”‚   â”œâ”€â”€ ml_attacks.py      # ML-based privacy attacks
â”‚   â””â”€â”€ disclosure_risk.py # Disclosure risk assessment
â”œâ”€â”€ diversity/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ coverage.py        # Coverage of the original data space
â”‚   â”œâ”€â”€ novelty.py         # Generation of novel but plausible samples
â”‚   â””â”€â”€ mode_collapse.py   # Detection of mode collapse issues
â””â”€â”€ common/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ metrics.py         # Base classes for metrics
    â”œâ”€â”€ data_preparation.py # Common data preparation utilities
    â”œâ”€â”€ reporting.py       # Reporting utilities
    â””â”€â”€ visualization.py   # Visualization utilities
```